{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Boosting Decision Stumps\n",
    "\n",
    "In this notebook, you will use [scikit-learn's decision trees](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) \n",
    "and implement the AdaBoost algorithm for binary classification. In particular, you will\n",
    "\n",
    "1. Implement AdaBoost and evaluate its performance on a toy dataset\n",
    "2. Investigate the effect of number of iterations/tree depth\n",
    "\n",
    "The exercise is mostly based on the lecture and the following book:\n",
    "\n",
    "T. Hastie, R. Tibshirani, and J. Friedman: [*The Elements for Statistical Learning*](http://statweb.stanford.edu/~tibs/ElemStatLearn/), 2001\n",
    "\n",
    "As usual, some setup first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.tree\n",
    "import sklearn.base\n",
    "import sklearn.metrics\n",
    "import sklearn.ensemble\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is an example from [scikit-learn's datasets submodule](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_hastie_10_2.html) which was introduced by [Hastie, Tibshirani and Friedman]. It has 10 features, which are standard independent Gaussian. The deterministic target Y is defined with:\n",
    "\n",
    "```   y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification dataset\n",
    "def create_dataset():\n",
    "    num_samples=12000\n",
    "    n_train = 2000\n",
    "    \n",
    "    X, y = sklearn.datasets.make_hastie_10_2(n_samples=num_samples, \n",
    "                                             random_state=1)\n",
    "    X_train = X[:n_train]\n",
    "    y_train = y[:n_train]\n",
    "    X_test = X[n_train:]\n",
    "    y_test = y[n_train:]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBooster(object):\n",
    "    \n",
    "    def __init__(self, weak_learner, iterations):\n",
    "        '''\n",
    "        weak_learner: sklearn classifier - G_m\n",
    "        iterations: number of iteration to train - M\n",
    "        '''\n",
    "        self.weak_learner = weak_learner\n",
    "        self.iterations = iterations\n",
    "        \n",
    "        # 1. Initialize some more lists\n",
    "        self.classifiers = list() # trained classifiers G_m\n",
    "        self.alphas = list()      # classifier weights       \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # YOUR TURN\n",
    "        # 1. initialize the observation weights\n",
    "        w = np.ones_like(y) / X.shape[0]\n",
    "        \n",
    "        # 2. For m=1 to M\n",
    "        for it in range(self.iterations):\n",
    "            # hint: Use sklearn.base.clone method to copy the \n",
    "            #       classifier in each iteration. Only 'clones'\n",
    "            #       the class, not the model. \n",
    "\n",
    "            # (a) Fit a classifier\n",
    "            tmp_learner = sklearn.base.clone(self.weak_learner)\n",
    "            tmp_learner.fit(X, y, sample_weight=w)\n",
    "            self.classifiers.append(tmp_learner)\n",
    "            \n",
    "            # (b) Compute error\n",
    "            y_pred = tmp_learner.predict(X)\n",
    "            error = np.sum(w * (y != y_pred))\n",
    "            error /= np.sum(w)\n",
    "\n",
    "            # (c) Compute classifier weight\n",
    "            alpha = np.log((1 - error) / error)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            # (d) Update observation weights\n",
    "            w *= np.exp(alpha * (y != y_pred))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Compute predictions by looping over all classifiers \n",
    "        # and adding weighted predictions. \n",
    "        # The actual output should be \n",
    "        # -1 if y_i <= 0 and 1 otherwise\n",
    "    \n",
    "        # YOUR TURN\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for alpha, c in zip(self.alphas, self.classifiers):\n",
    "            tmp_pred = c.predict(X)\n",
    "            pred += alpha * tmp_pred\n",
    "        pred[pred > 0] = 1\n",
    "        pred[pred <= 0] = -1\n",
    "        return pred\n",
    "    \n",
    "    def staged_predict(self, X):\n",
    "        # returns predictions for X for all iterations \n",
    "        # this is for convenience to simplify computing\n",
    "        # train/test error for all iterations\n",
    "\n",
    "        staged_predictions = []\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for alpha, c in zip(self.alphas, self.classifiers):\n",
    "            tmp_pred = c.predict(X)\n",
    "            pred += alpha * tmp_pred\n",
    "            \n",
    "            staged_prediction = pred.copy()\n",
    "            staged_prediction[staged_prediction > 0] = 1\n",
    "            staged_prediction[staged_prediction <= 0] = -1            \n",
    "            staged_predictions.append(staged_prediction)\n",
    "            \n",
    "        return staged_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your implementation\n",
    "\n",
    "Now, you have a working implementation of the AdaBoost algorithm. To test your algorithm for correctness you can compare to [sklearns AdaBoost implementation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) which should yield the same output as your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to test whether your implementation is correct\n",
    "\n",
    "weak_learner = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "adaboost = AdaBooster(weak_learner=weak_learner, iterations=400)\n",
    "sk_adaboost = sklearn.ensemble.AdaBoostClassifier(weak_learner, \n",
    "                                                  algorithm=\"SAMME\", \n",
    "                                                  n_estimators=400)\n",
    "\n",
    "# Get data\n",
    "X_train, y_train, X_test, y_test = create_dataset()\n",
    "\n",
    "# Fit model\n",
    "adaboost.fit(X_train, y_train)\n",
    "sk_adaboost.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "pred = adaboost.predict(X_test)\n",
    "pred_sk = sk_adaboost.predict(X_test)\n",
    "\n",
    "print(\"Difference: %d\" % np.sum([pred != pred_sk]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground\n",
    "\n",
    "Next, you can play around with AdaBoost's hyperparameters. Here are some suggestions:\n",
    "\n",
    " * Investigate the number of iterations that are needed to start overfitting.\n",
    " * Add some noise on the targets and try again.\n",
    " * Investigate the effect of the tree depth.\n",
    " \n",
    "Before you investigate the model it will be helpful to prepare a function that generates a plot showing\n",
    "test/train performance over number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_test_error(model, X_train, y_train, X_test, y_test):\n",
    "    ''' Shows a plot with train/test error over iterations\n",
    "\n",
    "    model (AdaBooster): trained AdaBoost algorithm\n",
    "    X_train (array): [#samples, #features]\n",
    "    y_train (array): [#samples, ]\n",
    "    X_test (array): [#samples, #features]\n",
    "    y_test (array): [#samples, ]\n",
    "    '''\n",
    "    # Loop over all predictions and compute accuracy\n",
    "    y_pred_train = model.staged_predict(X_train)\n",
    "    training_errors = [1-sklearn.metrics.accuracy_score(y_train, y_pred) \n",
    "                       for y_pred in y_pred_train]\n",
    "    \n",
    "    y_pred_test = model.staged_predict(X_test)\n",
    "    test_errors = [1-sklearn.metrics.accuracy_score(y_test, y_pred) \n",
    "                   for y_pred in y_pred_test]\n",
    "    \n",
    "    # Plot results\n",
    "    x_values = np.arange(1, len(training_errors) + 1)\n",
    "\n",
    "    plt.plot(x_values, training_errors, label=\"training\")\n",
    "    plt.plot(x_values, test_errors, label=\"test\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.ylim([0, 1.1])\n",
    "    \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate overfitting and find out number of iterations \n",
    "# needed to overfit\n",
    "# --> The train and test error diverge after roughly ~100 \n",
    "# iterations. While the training accuracy gets up to 1 \n",
    "# (after ~2000 iterations), the test accuracy does not increase.\n",
    "\n",
    "# YOUR TURN\n",
    "\n",
    "# Get data\n",
    "X_train, y_train, X_test, y_test = create_dataset()\n",
    "\n",
    "# Instantiate model\n",
    "weak_learner = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "adaboost = AdaBooster(weak_learner=weak_learner, iterations=3000)\n",
    "\n",
    "# Fit model\n",
    "adaboost.fit(X_train, y_train)\n",
    "pred = adaboost.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "plot_train_test_error(adaboost, X_train, y_train, X_test, y_test)\n",
    "plt.ylim([0, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some noise on the data and try again\n",
    "# --> Changing 0.01 of all y-values does not harm the prediction, \n",
    "# a higher noise decreases training accuracy \n",
    "\n",
    "# Get data\n",
    "X_train, y_train, X_test, y_test = create_dataset()\n",
    "swap_ratio = 0.2\n",
    "swap_idx = list(range(0, y_train.shape[0]))\n",
    "np.random.shuffle(swap_idx)\n",
    "y_train[swap_idx[:int(y_train.shape[0]*swap_ratio)]] *= -1\n",
    "\n",
    "# Instantiate model\n",
    "weak_learner = sklearn.tree.DecisionTreeClassifier(max_depth=1)\n",
    "adaboost = AdaBooster(weak_learner=weak_learner, iterations=1000)\n",
    "\n",
    "# Fit model\n",
    "adaboost.fit(X_train, y_train)\n",
    "pred = adaboost.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "plot_train_test_error(adaboost, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the effect of the tree depth\n",
    "# Outcome: Using simple learners allow to still learn a complex function. \n",
    "# Using a stronger base model increases the gap between train/test \n",
    "# performance\n",
    "\n",
    "# Get data\n",
    "X_train, y_train, X_test, y_test = create_dataset()\n",
    "\n",
    "# Instantiate model\n",
    "weak_learner = sklearn.tree.DecisionTreeClassifier(max_depth=10)\n",
    "adaboost = AdaBooster(weak_learner=weak_learner, iterations=400)\n",
    "\n",
    "# Fit model\n",
    "adaboost.fit(X_train, y_train)\n",
    "pred = adaboost.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "plot_train_test_error(adaboost, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
