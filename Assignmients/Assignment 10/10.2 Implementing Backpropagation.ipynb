{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Implementing Backpropagation\n",
    "\n",
    "In this exercise we will implement a simple multi-layer perceptron for the classic dataset of housing price prediction in Boston. We begin with loading and splitting the data, as well as normalizing the features. This is an important preprocessing step and should almost always be done!\n",
    "\n",
    "By transforming each feature to $f' = \\frac{f - \\mu_f}{\\sigma_f}$, where $\\mu_f$ is the mean and $\\sigma_f$ the standard deviation for feature $f$ in our sample set, we make sure that they are centered around zero and in comparable ranges.\n",
    "\n",
    "**Q 10.2.1: What would a standard deviation of zero mean for a feature?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "# Split the dataset in training and validation set\n",
    "n_training_samples = 300\n",
    "X_train = boston.data[:n_training_samples]\n",
    "y_train = boston.target[:n_training_samples].reshape((-1, 1))\n",
    "X_valid = boston.data[n_training_samples:]\n",
    "y_valid = boston.target[n_training_samples:].reshape((-1, 1))\n",
    "\n",
    "# Normalize the features by centering them around the mean \n",
    "# dividing by the standard deviation. Note that we calculate\n",
    "# the statistical moments only on the training set!\n",
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, we can set up the network. We will stick to the architecture from the lecture: one hidden layer with a rectified linear activation and the output layer with linear output since this is a regression task. After setting this up, we need to be able to correctly pass data forward and backward through the network. Below the code stub here, you will find a gradient checking procedure to verify your implementation before we turn to the optimization in the next part.\n",
    "\n",
    "**Task a) Setting up an MLP:** \n",
    "  * Implement the missing pieces of the architecture for one hidden layer with 50 units and rectified linear activation and one linear output unit\n",
    "  * Implement the forward and backward pass for this architecture.\n",
    "  * Verify your implementation with the gradient checking procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network architecture\n",
    "n_input_units = X_train.shape[1]\n",
    "n_hidden_units = 50\n",
    "n_output_units = 1\n",
    "\n",
    "# Implement the shapes in terms of the above definitions.\n",
    "# For verification and broadcasting reasons the bias \n",
    "# should have 1 row, i.e. the shape should be (1, x)\n",
    "# with x determined by you.\n",
    "\n",
    "# Every layer is a matrix multiplication plus a bias, so\n",
    "# the shapes must match those operations.\n",
    "hidden_weights_shape = (1, 1) # TODO\n",
    "output_weights_shape = (1, 1) # TODO\n",
    "hidden_bias_shape = (1, 1) # TODO\n",
    "output_bias_shape = (1, 1) # TODO\n",
    "\n",
    "# For the optimization with gradient descent, we have to pick an\n",
    "# initial guess for the solution.\n",
    "def init_weights():\n",
    "    # We initialize the weights with a Gaussian distribution\n",
    "    hidden_weights = np.random.standard_normal(hidden_weights_shape) * 0.1\n",
    "    output_weights = np.random.standard_normal(output_weights_shape) * 0.1\n",
    "\n",
    "    # The bias is usually initialized to zero\n",
    "    hidden_bias = np.zeros(hidden_bias_shape)\n",
    "    output_bias = np.zeros(output_bias_shape)\n",
    "\n",
    "    return hidden_weights, output_weights, hidden_bias, output_bias\n",
    "    \n",
    "hidden_weights, output_weights, hidden_bias, output_bias = init_weights()\n",
    "    \n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    # It's important to use the elementwise max function in numpy\n",
    "    return np.maximum(0., x)\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "# The forward pass for this neural network\n",
    "# The output for every layer should be computed in\n",
    "# two steps: before and after applying the activation\n",
    "# function.\n",
    "def forward_pass(X):\n",
    "    hidden_linear = np.nan # TODO\n",
    "    hidden_act = np.nan # TODO\n",
    "    \n",
    "    output_linear = np.nan # TODO\n",
    "    output_act = np.nan # TODO\n",
    "    \n",
    "    # We collect the activations in a dictionary here,\n",
    "    # because we need them for the backward pass\n",
    "    activations = {\n",
    "        'hidden_linear': hidden_linear,\n",
    "        'hidden_act': hidden_act,\n",
    "        'output_linear': output_linear,\n",
    "        'output_act': output_act\n",
    "    }\n",
    "    \n",
    "    # The prediction is in output_act\n",
    "    return output_act, activations\n",
    "\n",
    "\n",
    "# Loss function\n",
    "# Implement the mean square error function\n",
    "def squared_error(y_predicted, y):\n",
    "    return np.nan # TODO\n",
    "\n",
    "\n",
    "# This completes everything we need for the forward pass\n",
    "# If you are unsure about your implementation, feel\n",
    "# free to pass random values through and check the output\n",
    "# shapes\n",
    "\n",
    "# For the backward pass, we will need the\n",
    "# derivatives of the activation functions and the loss function\n",
    "# The gradients are defined in the slides, we just need to match\n",
    "# the input shapes.\n",
    "def relu_grad(x):\n",
    "    return np.nan # TODO\n",
    "\n",
    "def linear_grad(x):\n",
    "    return np.nan # TODO\n",
    "\n",
    "# The error gradient wrt the predicted y value\n",
    "def squared_error_grad(y_hat, y):\n",
    "    n_samples = float(y.shape[0])\n",
    "    return np.nan # TODO\n",
    "    \n",
    "\n",
    "# The backward pass is responsible for calculating the derivatives\n",
    "# of the loss with respect to the weights and biases of each layer.\n",
    "# It also helps to think about the shapes (again!)\n",
    "def  backward_pass(X, y, activations):\n",
    "    # For most of the gradients you already filled out helper functions. Use them!\n",
    "    \n",
    "    # Wherever you need the elements of the forward pass, you will find them\n",
    "    # in the activations dictionary. See the forward pass for the keys or\n",
    "    # print it to find out.\n",
    "    \n",
    "    # The first thing to calculate is the gradient of the error function wrt\n",
    "    # the prediction the network made\n",
    "    error_grad = np.nan # TODO\n",
    "    \n",
    "    # Now we can go backwards layer by layer\n",
    "    # First derive wrt the activation function, because it is the last thing we did in\n",
    "    # the forward pass\n",
    "    output_act_grad = np.nan # TODO\n",
    "    \n",
    "    # Two objects contributed to Xw + b: the weights and the bias\n",
    "    # These are the parameters of our network and will let us update\n",
    "    # the network later.\n",
    "    output_weights_grad = np.nan # TODO\n",
    "    \n",
    "    # Remember that the bias was broadcasted from its shape (1, k) to \n",
    "    # (n, k) with n being the number of samples. For the correct gradient\n",
    "    # you need to get back to (1, k) by summing up.\n",
    "    output_bias_grad = np.nan # TODO\n",
    "    \n",
    "    # The last thing to do for the output layer is to take the derivative wrt\n",
    "    # its input, so we can backpropagate the errors the previous layers need\n",
    "    # to fix.\n",
    "    output_inp_grad = np.nan # TODO\n",
    "\n",
    "    # From here on, everything is the same as for the output layer, except\n",
    "    # for the changed activation function\n",
    "    hidden_act_grad = np.nan # TODO\n",
    "    hidden_weights_grad = np.nan # TODO\n",
    "    hidden_bias_grad = np.nan # TODO\n",
    "    \n",
    "    # These are the gradients we care about in the end, because\n",
    "    # we need them to update our network\n",
    "    gradients = (\n",
    "        output_weights_grad,\n",
    "        output_bias_grad,\n",
    "        hidden_weights_grad,\n",
    "        hidden_bias_grad\n",
    "    )\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q 10.2.2: Remember how the regularization for polynomial models worked. How could you apply it here?**\n",
    "\n",
    "#### Gradient Checking\n",
    "\n",
    "Although everything is in place to turn our attention to training networks, it is always a good idea to verify the gradients of your network. Although there are nowadays lots of frameworks to compute the gradients for you, we use a (central) finite difference approximation to make sure the implementation is correct:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{ \\mathcal{L}(\\theta + \\epsilon, X) - \\mathcal{L}(\\theta - \\epsilon, X)}{2 \\epsilon} \\approx \\frac{\\partial \\mathcal{L}(\\theta, X)}{\\partial \\theta}\n",
    "\\end{equation}\n",
    "\n",
    "Looks familiar? For vanishing $\\epsilon$ the left term is the definition of the derivative of the loss function $\\mathcal{L}$ with respect to $\\theta$. With a small enough $\\epsilon$, we can validate the backward pass. If the code below does not print anything, you're golden!\n",
    "\n",
    "Note though, that this method isn't perfect and the finite precision of floating point numbers sometimes interferes even with correct code. If you think this happens in your case, try changing the threshold and the epsilon, but in almost every case the current settings should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs detected\n",
      "Output weight gradient incorrect!\n",
      "NaNs detected\n",
      "Output bias gradient incorrect!\n",
      "NaNs detected\n",
      "Hidden weight gradient incorrect!\n",
      "NaNs detected\n",
      "Hidden bias gradient incorrect!\n"
     ]
    }
   ],
   "source": [
    "def check_gradients():\n",
    "        X_test = np.random.standard_normal((30, n_input_units))\n",
    "        y_test = np.random.standard_normal((30, n_output_units))\n",
    "    \n",
    "        y_pred, act = forward_pass(X_test)\n",
    "        owg, obg, hwg, hbg = backward_pass(X_test, y_test, act)\n",
    "        \n",
    "        def check_param(param, grad_est, epsilon=1e-7, error_threshold=1e-3):\n",
    "            if np.any(np.isnan(param)):\n",
    "                print('Parameters not implemented (NaN).')\n",
    "                return False\n",
    "            \n",
    "            orig = param.copy()\n",
    "            for i in range(param.shape[0]):\n",
    "                for j in range(param.shape[1]):\n",
    "                    if np.any(np.isnan(grad_est)):\n",
    "                        print('NaNs detected')\n",
    "                        return False\n",
    "                    \n",
    "                    param[:] = orig.copy()\n",
    "                    param[i, j] = orig[i, j] + epsilon\n",
    "                    loss_plus = squared_error(forward_pass(X_test)[0], y_test)\n",
    "                    param[i, j] = orig[i, j] - epsilon\n",
    "                    loss_minus = squared_error(forward_pass(X_test)[0], y_test)\n",
    "                    g = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "                    \n",
    "                    if np.abs(g - grad_est[i, j]) > error_threshold:\n",
    "                        print('Finite difference:', g)\n",
    "                        print('Yours: ', grad_est[i, j])\n",
    "                        return False\n",
    "                    \n",
    "            return True\n",
    "        \n",
    "        if not check_param(output_weights, owg):\n",
    "            print('Output weight gradient incorrect!')\n",
    "        if not check_param(output_bias, obg):\n",
    "            print('Output bias gradient incorrect!')\n",
    "        if not check_param(hidden_weights, hwg):\n",
    "            print('Hidden weight gradient incorrect!')\n",
    "        if not check_param(hidden_bias, hbg):\n",
    "            print('Hidden bias gradient incorrect!')            \n",
    "\n",
    "check_gradients()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost able to actually train our network. There is just one last thing missing:\n",
    "\n",
    "**Task b) Updating the weights: ** Complete the training loop and implement the Gradient Descent updates using the gradients from the previous task.\n",
    "\n",
    "It is also highly encouraged that you play around with the number of hidden units, weight initialization, epochs and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "\tTraining loss: nan\n",
      "\tValidation loss: nan\n",
      "Epoch 50\n",
      "\tTraining loss: nan\n",
      "\tValidation loss: nan\n",
      "Epoch 100\n",
      "\tTraining loss: nan\n",
      "\tValidation loss: nan\n",
      "Epoch 150\n",
      "\tTraining loss: nan\n",
      "\tValidation loss: nan\n",
      "Epoch 200\n",
      "\tTraining loss: nan\n",
      "\tValidation loss: nan\n",
      "Epoch 250\n",
      "\tTraining loss: nan\n",
      "\tValidation loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f45a99b25c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtUAAAHVCAYAAAA+Wwf2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAH85JREFUeJzt3X+wXWV97/HPFwJEgfIr/BDibagyA6EgP444t0ABRYRSCNLYijhQqkNhsNRLnSG3pQpoO+BwhVq5WuTHULWmjI4lt5YyFKJge4uchBgE5CZqHAKUhgBRBIoHn/tHtpmAhyTkOTknCa/XzJmz11rPXvs5eYi8Wa69T7XWAgAArL8tJnoCAACwqRPVAADQSVQDAEAnUQ0AAJ1ENQAAdBLVAADQSVQDAEAnUQ0AAJ1ENQAAdJo00RNYH1OmTGnTpk2b6GkAALAZmzdv3hOttV3XZewmGdXTpk3L8PDwRE8DAIDNWFX9aF3Huv0DAAA6iWoAAOgkqgEAoNMmeU81AMDG7Gc/+1mWLl2a559/fqKnwjqYPHlypk6dmq222mq9zyGqAQDG2NKlS7P99ttn2rRpqaqJng5r0FrL8uXLs3Tp0uy9997rfR63fwAAjLHnn38+u+yyi6DeBFRVdtlll+7/V0FUAwBsAIJ60zEWayWqAQCgk6gGANjMLF++PAcddFAOOuig7LHHHtlrr71Wbb/wwgvrdI6zzjorDz300BrHXH311fnSl740FlPOEUcckQULFozJuSaCNyoCAGxmdtlll1WBevHFF2e77bbLRz7ykZeMaa2ltZYtthj9GusNN9yw1tc577zz+ie7mRDVAAAb0CX/5/488OiPx/Sc0/f8lXzspP1f9fMWL16ck08+OQcffHDuvffe3Hbbbbnkkksyf/78PPfcc/m93/u9fPSjH02y8srxZz7zmfz6r/96pkyZknPOOSe33HJLXv/61+fmm2/ObrvtlosuuihTpkzJhz/84RxxxBE54ogjcscdd2TFihW54YYb8hu/8Rv56U9/mjPOOCMPPvhgpk+fniVLluTaa6/NQQcd9Irz/OIXv5jLL788rbWcfPLJ+cu//MuMjIzkrLPOyoIFC9Jay9lnn53zzz8/V155ZT7/+c9n0qRJOfDAA/PFL35xvf9ce4hqAIDXkO9973v527/92wwNDSVJLrvssuy8884ZGRnJMccck5kzZ2b69Okvec6KFSty1FFH5bLLLssFF1yQ66+/PrNmzfqlc7fW8u1vfztz5szJpZdemn/+53/OX//1X2ePPfbIV7/61XznO9/JIYccssb5LV26NBdddFGGh4ezww475Nhjj80//uM/Ztddd80TTzyR++67L0ny9NNPJ0k++clP5kc/+lG23nrrVfsmgqgGANiA1ueK8ob0pje9aVVQJ8mXv/zlXHfddRkZGcmjjz6aBx544Jei+nWve11OOOGEJMmhhx6au+66a9Rzn3rqqavGLFmyJEnyrW99KxdeeGGS5C1veUv233/Nfx5333133v72t2fKlClJkve973258847c+GFF+ahhx7K+eefnxNPPDHHHXdckmT//ffP+9///syYMSOnnHLKq/zTGDveqAgA8Bqy7bbbrnq8aNGi/NVf/VXuuOOOLFy4MMcff/yon9e89dZbr3q85ZZbZmRkZNRzb7PNNmsds7522WWXLFy4MEceeWSuvvrq/OEf/mGS5NZbb80555yTe+65J4cddlhefPHFMX3ddSWqAQBeo3784x9n++23z6/8yq/ksccey6233jrmr3H44YfnpptuSpLcd999eeCBB9Y4/m1ve1vmzp2b5cuXZ2RkJLNnz85RRx2VZcuWpbWW97znPbn00kszf/78vPjii1m6dGne/va355Of/GSeeOKJPPvss2P+M6wLt38AALxGHXLIIZk+fXr23Xff/Oqv/moOP/zwMX+NP/qjP8oZZ5yR6dOnr/raYYcdXnH81KlT8/GPfzxHH310Wms56aSTcuKJJ2b+/Pn5wAc+kNZaqiqXX355RkZG8r73vS8/+clP8vOf/zwf+chHsv3224/5z7AuqrU2IS/cY2hoqA0PD0/0NAAARvXggw9mv/32m+hpbBRGRkYyMjKSyZMnZ9GiRTnuuOOyaNGiTJq0cV3bHW3Nqmpea23oFZ7yEhvXTwMAwGblmWeeyTve8Y6MjIyktZa/+Zu/2eiCeixsfj8RAAAbjR133DHz5s2b6GlscN6oCAAAnUQ1AAB0EtUAANBJVAMAQCdRDQCwmTnmmGN+6Re5XHXVVTn33HPX+LztttsuSfLoo49m5syZo445+uijs7aPNr7qqqte8ktYfuu3fitPP/30ukx9jS6++OJcccUV3efZEEQ1AMBm5rTTTsvs2bNfsm/27Nk57bTT1un5e+65Z77yla+s9+u/PKr/6Z/+KTvuuON6n29T4CP1AAA2pFtmJf9x39iec48DkhMue8XDM2fOzEUXXZQXXnghW2+9dZYsWZJHH300Rx55ZJ555pnMmDEjTz31VH72s5/lE5/4RGbMmPGS5y9ZsiS//du/ne9+97t57rnnctZZZ+U73/lO9t133zz33HOrxp177rm555578txzz2XmzJm55JJL8ulPfzqPPvpojjnmmEyZMiVz587NtGnTMjw8nClTpuRTn/pUrr/++iTJBz/4wXz4wx/OkiVLcsIJJ+SII47Iv/3bv2WvvfbKzTffnNe97nWv+DMuWLAg55xzTp599tm86U1vyvXXX5+ddtopn/70p/O5z30ukyZNyvTp0zN79ux885vfzB//8R8nSaoqd95555j/5kVXqgEANjM777xzDjvssNxyyy1JVl6l/t3f/d1UVSZPnpyvfe1rmT9/fubOnZs/+ZM/yZp+w/ZnP/vZvP71r8+DDz6YSy655CWfOf0Xf/EXGR4ezsKFC/PNb34zCxcuzPnnn58999wzc+fOzdy5c19yrnnz5uWGG27I3XffnX//93/P5z//+dx7771JkkWLFuW8887L/fffnx133DFf/epX1/gznnHGGbn88suzcOHCHHDAAbnkkkuSJJdddlnuvffeLFy4MJ/73OeSJFdccUWuvvrqLFiwIHfdddcaY319uVINALAhreGK8ob0i1tAZsyYkdmzZ+e6665LkrTW8qd/+qe58847s8UWW+SRRx7J448/nj322GPU89x55505//zzkyQHHnhgDjzwwFXHbrrpplxzzTUZGRnJY489lgceeOAlx1/uW9/6Vt797ndn2223TZKceuqpueuuu3LyySdn7733zkEHHZQkOfTQQ7NkyZJXPM+KFSvy9NNP56ijjkqSnHnmmXnPe96zao6nn356TjnllJxyyilJksMPPzwXXHBBTj/99Jx66qmZOnXquvwRviquVAMAbIZmzJiR22+/PfPnz8+zzz6bQw89NEnypS99KcuWLcu8efOyYMGC7L777nn++edf9fl/+MMf5oorrsjtt9+ehQsX5sQTT1yv8/zCNttss+rxlltumZGRkfU6z9e//vWcd955mT9/ft761rdmZGQks2bNyrXXXpvnnnsuhx9+eL73ve+t9zxfiagGANgMbbfddjnmmGPyB3/wBy95g+KKFSuy2267ZauttsrcuXPzox/9aI3n+c3f/M383d/9XZLku9/9bhYuXJgk+fGPf5xtt902O+ywQx5//PFVt5okyfbbb5+f/OQnv3SuI488Mv/wD/+QZ599Nj/96U/zta99LUceeeSr/tl22GGH7LTTTrnrrruSJF/4whdy1FFH5ec//3kefvjhHHPMMbn88suzYsWKPPPMM/n+97+fAw44IBdeeGHe+ta3bpCodvsHAMBm6rTTTsu73/3ul3wSyOmnn56TTjopBxxwQIaGhrLvvvuu8RznnntuzjrrrOy3337Zb7/9Vl3xfstb3pKDDz44++67b974xjfm8MMPX/Wcs88+O8cff/yqe6t/4ZBDDsnv//7v57DDDkuy8o2KBx988Bpv9XglN95446o3Kv7ar/1abrjhhrz44ot5//vfnxUrVqS1lvPPPz877rhj/vzP/zxz587NFltskf333z8nnHDCq369tak13Zi+sRoaGmpr+3xEAICJ8uCDD2a//fab6GnwKoy2ZlU1r7U2tC7Pd/sHAAB0EtUAANBJVAMAbACb4i22r1VjsVaiGgBgjE2ePDnLly8X1puA1lqWL1+eyZMnd53Hp38AAIyxqVOnZunSpVm2bNlET4V1MHny5O5fCCOqAQDG2FZbbZW99957oqfBOHL7BwAAdBLVAADQSVQDAEAnUQ0AAJ1ENQAAdBLVAADQSVQDAEAnUQ0AAJ1ENQAAdBLVAADQSVQDAEAnUQ0AAJ1ENQAAdBLVAADQSVQDAEAnUQ0AAJ3GJKqr6viqeqiqFlfVrFGOb1NVfz84fndVTXvZ8f9WVc9U1UfGYj4AADCeuqO6qrZMcnWSE5JMT3JaVU1/2bAPJHmqtfbmJFcmufxlxz+V5JbeuQAAwEQYiyvVhyVZ3Fr7QWvthSSzk8x42ZgZSW4cPP5KkndUVSVJVZ2S5IdJ7h+DuQAAwLgbi6jeK8nDq20vHewbdUxrbSTJiiS7VNV2SS5McsnaXqSqzq6q4aoaXrZs2RhMGwAAxsZEv1Hx4iRXttaeWdvA1to1rbWh1trQrrvuuuFnBgAA62jSGJzjkSRvXG176mDfaGOWVtWkJDskWZ7kbUlmVtUnk+yY5OdV9Xxr7TNjMC8AABgXYxHV9yTZp6r2zsp4fm+S971szJwkZyb5v0lmJrmjtdaSHPmLAVV1cZJnBDUAAJua7qhurY1U1YeS3JpkyyTXt9bur6pLkwy31uYkuS7JF6pqcZInszK8AQBgs1ArLxhvWoaGhtrw8PBETwMAgM1YVc1rrQ2ty9iJfqMiAABs8kQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0EtUAANBJVAMAQCdRDQAAnUQ1AAB0GpOorqrjq+qhqlpcVbNGOb5NVf394PjdVTVtsP+dVTWvqu4bfH/7WMwHAADGU3dUV9WWSa5OckKS6UlOq6rpLxv2gSRPtdbenOTKJJcP9j+R5KTW2gFJzkzyhd75AADAeBuLK9WHJVncWvtBa+2FJLOTzHjZmBlJbhw8/kqSd1RVtdbuba09Oth/f5LXVdU2YzAnAAAYN2MR1XsleXi17aWDfaOOaa2NJFmRZJeXjfmdJPNba/812otU1dlVNVxVw8uWLRuDaQMAwNjYKN6oWFX7Z+UtIX/4SmNaa9e01oZaa0O77rrr+E0OAADWYiyi+pEkb1xte+pg36hjqmpSkh2SLB9sT03ytSRntNa+PwbzAQCAcTUWUX1Pkn2qau+q2jrJe5PMedmYOVn5RsQkmZnkjtZaq6odk3w9yazW2r+OwVwAAGDcdUf14B7pDyW5NcmDSW5qrd1fVZdW1cmDYdcl2aWqFie5IMkvPnbvQ0nenOSjVbVg8LVb75wAAGA8VWttoufwqg0NDbXh4eGJngYAAJuxqprXWhtal7EbxRsVAQBgUyaqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCg05hEdVUdX1UPVdXiqpo1yvFtqurvB8fvrqppqx37n4P9D1XVu8ZiPgAAMJ66o7qqtkxydZITkkxPclpVTX/ZsA8keaq19uYkVya5fPDc6Unem2T/JMcn+d+D8wEAwCZjLK5UH5ZkcWvtB621F5LMTjLjZWNmJLlx8PgrSd5RVTXYP7u19l+ttR8mWTw4HwAAbDLGIqr3SvLwattLB/tGHdNaG0myIsku6/jcJElVnV1Vw1U1vGzZsjGYNgAAjI1N5o2KrbVrWmtDrbWhXXfddaKnAwAAq4xFVD+S5I2rbU8d7Bt1TFVNSrJDkuXr+FwAANiojUVU35Nkn6rau6q2zso3Hs552Zg5Sc4cPJ6Z5I7WWhvsf+/g00H2TrJPkm+PwZwAAGDcTOo9QWttpKo+lOTWJFsmub61dn9VXZpkuLU2J8l1Sb5QVYuTPJmV4Z3BuJuSPJBkJMl5rbUXe+cEAADjqVZeMN60DA0NteHh4YmeBgAAm7GqmtdaG1qXsZvMGxUBAGBjJaoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKBTV1RX1c5VdVtVLRp83+kVxp05GLOoqs4c7Ht9VX29qr5XVfdX1WU9cwEAgInSe6V6VpLbW2v7JLl9sP0SVbVzko8leVuSw5J8bLX4vqK1tm+Sg5McXlUndM4HAADGXW9Uz0hy4+DxjUlOGWXMu5Lc1lp7srX2VJLbkhzfWnu2tTY3SVprLySZn2Rq53wAAGDc9Ub17q21xwaP/yPJ7qOM2SvJw6ttLx3sW6WqdkxyUlZe7R5VVZ1dVcNVNbxs2bK+WQMAwBiatLYBVfUvSfYY5dCfrb7RWmtV1V7tBKpqUpIvJ/l0a+0HrzSutXZNkmuSZGho6FW/DgAAbChrjerW2rGvdKyqHq+qN7TWHquqNyT5z1GGPZLk6NW2pyb5xmrb1yRZ1Fq7ap1mDAAAG5ne2z/mJDlz8PjMJDePMubWJMdV1U6DNygeN9iXqvpEkh2SfLhzHgAAMGF6o/qyJO+sqkVJjh1sp6qGquraJGmtPZnk40nuGXxd2lp7sqqmZuUtJNOTzK+qBVX1wc75AADAuKvWNr3bk4eGhtrw8PBETwMAgM1YVc1rrQ2ty1i/UREAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6CSqAQCgk6gGAIBOohoAADqJagAA6NQV1VW1c1XdVlWLBt93eoVxZw7GLKqqM0c5PqeqvtszFwAAmCi9V6pnJbm9tbZPktsH2y9RVTsn+ViStyU5LMnHVo/vqjo1yTOd8wAAgAnTG9Uzktw4eHxjklNGGfOuJLe11p5srT2V5LYkxydJVW2X5IIkn+icBwAATJjeqN69tfbY4PF/JNl9lDF7JXl4te2lg31J8vEk/yvJs2t7oao6u6qGq2p42bJlHVMGAICxNWltA6rqX5LsMcqhP1t9o7XWqqqt6wtX1UFJ3tRa+x9VNW1t41tr1yS5JkmGhobW+XUAAGBDW2tUt9aOfaVjVfV4Vb2htfZYVb0hyX+OMuyRJEevtj01yTeS/PckQ1W1ZDCP3arqG621owMAAJuQ3ts/5iT5xad5nJnk5lHG3JrkuKraafAGxeOS3Npa+2xrbc/W2rQkRyT5f4IaAIBNUW9UX5bknVW1KMmxg+1U1VBVXZskrbUns/Le6XsGX5cO9gEAwGahWtv0bk8eGhpqw8PDEz0NAAA2Y1U1r7U2tC5j/UZFAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKCTqAYAgE6iGgAAOolqAADoJKoBAKBTtdYmeg6vWlUtS/KjiZ7Ha8SUJE9M9CTY4Kzza4N13vxZ49cG6zx+frW1tuu6DNwko5rxU1XDrbWhiZ4HG5Z1fm2wzps/a/zaYJ03Tm7/AACATqIaAAA6iWrW5pqJngDjwjq/NljnzZ81fm2wzhsh91QDAEAnV6oBAKCTqAYAgE6imlTVzlV1W1UtGnzf6RXGnTkYs6iqzhzl+Jyq+u6GnzHro2edq+r1VfX1qvpeVd1fVZeN7+xZk6o6vqoeqqrFVTVrlOPbVNXfD47fXVXTVjv2Pwf7H6qqd43nvHl11nedq+qdVTWvqu4bfH/7eM+dddfz93lw/L9V1TNV9ZHxmjMriWqSZFaS21tr+yS5fbD9ElW1c5KPJXlbksOSfGz1KKuqU5M8Mz7TZT31rvMVrbV9kxyc5PCqOmF8ps2aVNWWSa5OckKS6UlOq6rpLxv2gSRPtdbenOTKJJcPnjs9yXuT7J/k+CT/e3A+NjI965yVvyTkpNbaAUnOTPKF8Zk1r1bnOv/Cp5LcsqHnyi8T1STJjCQ3Dh7fmOSUUca8K8ltrbUnW2tPJbktK/8lnKraLskFST4xDnNl/a33OrfWnm2tzU2S1toLSeYnmToOc2btDkuyuLX2g8HazM7KtV7d6mv/lSTvqKoa7J/dWvuv1toPkywenI+Nz3qvc2vt3tbao4P99yd5XVVtMy6z5tXq+fucqjolyQ+zcp0ZZ6KaJNm9tfbY4PF/JNl9lDF7JXl4te2lg31J8vEk/yvJsxtshoyF3nVOklTVjklOysqr3Uy8ta7Z6mNaayNJViTZZR2fy8ahZ51X9ztJ5rfW/msDzZM+673OgwtcFya5ZBzmySgmTfQEGB9V9S9J9hjl0J+tvtFaa1W1zp+zWFUHJXlTa+1/vPy+Lsbfhlrn1c4/KcmXk3y6tfaD9ZslMBGqav+svFXguImeCxvExUmubK09M7hwzTgT1a8RrbVjX+lYVT1eVW9orT1WVW9I8p+jDHskydGrbU9N8o0k/z3JUFUtycp/nnarqm+01o4O424DrvMvXJNkUWvtqjGYLmPjkSRvXG176mDfaGOWDv7DaIcky9fxuWwcetY5VTU1ydeSnNFa+/6Gny7rqWed35ZkZlV9MsmOSX5eVc+31j6z4adN4vYPVpqTlW9eyeD7zaOMuTXJcVW10+CNa8clubW19tnW2p6ttWlJjkjy/wT1Rmu91zlJquoTWfk/3h8eh7my7u5Jsk9V7V1VW2flGw/nvGzM6ms/M8kdbeVv/pqT5L2DTxPYO8k+Sb49TvPm1VnvdR7csvX1JLNaa/86bjNmfaz3OrfWjmytTRv8+/iqJH8pqMeXqCZJLkvyzqpalOTYwXaqaqiqrk2S1tqTWXnv9D2Dr0sH+9h0rPc6D65y/VlWvht9flUtqKoPTsQPwUsN7qn8UFb+x8+DSW5qrd1fVZdW1cmDYddl5T2Xi7PyTcWzBs+9P8lNSR5I8s9JzmutvTjePwNr17POg+e9OclHB393F1TVbuP8I7AOOteZCebXlAMAQCdXqgEAoJOoBgCATqIaAAA6iWoAAOgkqgEAoJOoBgCATqIaAAA6/X9PFQSiTlYiwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 300\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# We keep track of training and validation error\n",
    "# for every episode.\n",
    "training_loss = np.ones(n_epochs) * np.nan\n",
    "valid_loss = np.ones(n_epochs) * np.nan\n",
    "\n",
    "hidden_weights, output_weights, hidden_bias, output_bias = init_weights()\n",
    "\n",
    "# The training loop\n",
    "for i in range(n_epochs):\n",
    "    # Use your previously implemented methods to\n",
    "    # Calculate the gradient for X_train\n",
    "\n",
    "    # TODO\n",
    "    \n",
    "    # We need to know how the network performs,\n",
    "    # so we should evaluate the loss on training and validation set\n",
    "    training_loss[i] = np.nan # TODO\n",
    "    valid_loss[i] = np.nan # TODO\n",
    "    \n",
    "    # Now implement the gradient descent update step\n",
    "    hidden_weights = np.nan # TODO \n",
    "    hidden_bias = np.nan # TODO \n",
    "    output_weights = np.nan # TODO \n",
    "    output_bias = np.nan # TODO \n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        print('Epoch {:d}'.format(i))\n",
    "        print('\\tTraining loss: {:.1f}'.format(training_loss[i]))\n",
    "        print('\\tValidation loss: {:.1f}'.format(valid_loss[i]))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(training_loss, label='Training loss')\n",
    "plt.plot(valid_loss, label='Validation loss')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q 10.2.3: What is the effect of rather small or large learning rates on the error?**\n",
    "\n",
    "** Q 10.2.4: Do you see a problem with using the same learning rate for all weights in the MLP?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
